{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import spacy as sp\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = sp.load('en_core_web_sm')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from spacy.matcher import PhraseMatcher\n",
    "\n",
    "matcher = PhraseMatcher(nlp.vocab, attr = \"LOWER\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Feature extraction uses phrasematcher from the spacy module to determine what events occured. \n",
    "e.g. red odu, recent weather events. \n",
    "The phrasematcher works based on tokenisation, this allows words of strings be distinguish from each other.\n",
    "\n",
    "E.g.\n",
    "\n",
    "\"red odu? na\" vs \"red odu? n\"\n",
    "\n",
    "\n",
    "The attr = \"LOWER\" allows for case insensitive matches\n",
    "\n",
    "Using the spacy 3.0 requires python 3.8(maybe higher version work as well) \n",
    "\n",
    "\n",
    "Note: Please check file type matches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocessing(df):\n",
    "    \n",
    "    \n",
    "    for i in np.arange(0, len(df.index)): # len(df.index) is the number of rows of df\n",
    "\n",
    "        \"\"\"\n",
    "        Code is not that important. This was initially done to avoid tokenising the \n",
    "        format specifier which would have an effect on the phrasematcher.\n",
    "\n",
    "        \"\"\"\n",
    "\n",
    "        df.loc[i, \"description\"] = str(df[\"description\"][i]).replace(\"\\n\", \" \")\n",
    "\n",
    "        df.loc[i, \"description\"] = str(df[\"description\"][i]).replace(\"\\r\", \" \")\n",
    "\n",
    "\n",
    "        df.loc[i, \"worklog\"] = str(df[\"worklog\"][i]).replace(\"\\n\", \" \")\n",
    "\n",
    "        df.loc[i, \"worklog\"] = str(df[\"worklog\"][i]).replace(\"\\r\", \" \")\n",
    "\n",
    "        df.loc[i, \"worklog\"] = str(df[\"worklog\"][i]).replace(\"\\t\", \" \")\n",
    "\n",
    "\n",
    "\n",
    "        # Remove multiple spaces\n",
    "\n",
    "        df.loc[i, \"description\"] = ' '.join( str(df[\"description\"][i]).split() ) #Allows for whitespace insensitive matches\n",
    "\n",
    "        df.loc[i, \"worklog\"] = ' '.join( str(df[\"worklog\"][i]).split() ) #Allows whitespace case insensitive matches\n",
    "        \n",
    "        \n",
    "        return (df[\"description\"][:], df[\"worklog\"][:]) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Output is binary matrix appended to the csv file for each feature in phrase_list\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "def NLP_one_hot_encoding(df, phrase_list, file_name, label_name):\n",
    "    \n",
    "\n",
    "    df.loc[:, \"description\"], df.loc[:, \"worklog\"] = preprocessing(df)\n",
    "    \n",
    "    column_list = [*df.columns, *phrase_list]\n",
    "    \n",
    "    \n",
    "    patterns = [nlp(text) for text in phrase_list]\n",
    "    \n",
    "    \n",
    "    for j in patterns:\n",
    "    \n",
    "\n",
    "        matcher.add(\"pattern_name\", None, *[j])\n",
    "\n",
    "        matches_found = np.array([])\n",
    "\n",
    "        feature_name = np.tile(0, (1, len(df.index)))   \n",
    "    \n",
    "    \n",
    "        for i in np.arange(0, len(df.index)): # len(df.index) is the number of rows of df \n",
    "\n",
    "            array_size = np.size(matches_found)\n",
    "\n",
    "            matches_found = np.append(matches_found, matcher( nlp ( str(df[label_name][i])) ) )\n",
    "\n",
    "            if(array_size < np.size(matches_found) ):\n",
    "                feature_name[0][i] = 1\n",
    "            \n",
    "    \n",
    "        df = pd.concat([df, pd.DataFrame(np.transpose(feature_name))], axis=1)\n",
    "        \n",
    "        \n",
    "        matcher.remove(\"pattern_name\")\n",
    "        \n",
    "     \n",
    "    df.columns =  column_list\n",
    "    \n",
    "    df.to_csv(file_name, index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Apply the one-hot encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(file_name)\n",
    "NLP_one_hot_encoding(df, [\"some_string\"], \"some_file_name\", \"description\")\n",
    "NLP_one_hot_encoding(df, [\"some_string\"], \"some_file_name\", \"worklog\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
